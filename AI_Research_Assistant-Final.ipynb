"""
═══════════════════════════════════════════════════════════════════════════
   AI RESEARCH ASSISTANT NAVIGATOR - JUPYTER NOTEBOOK
   
   Agentic AI Application for AWS & NVIDIA Hackathon
   - Llama-3.1-Nemotron-Nano-8B-v1 with Reasoning Mode
   - NeMo Retriever Embedding NIM
   - Deployed on Amazon SageMaker
═══════════════════════════════════════════════════════════════════════════
"""

# ═══════════════════════════════════════════════════════════════════════════
# SECTION 1: SETUP & PREREQUISITES
# ═══════════════════════════════════════════════════════════════════════════

print("AI Research Assistant Navigator - Initialization")
print("="*70)

# Install required packages
%pip install -q boto3 sagemaker opensearch-py requests-aws4auth
%pip install -q networkx plotly pandas numpy feedparser

import boto3
import sagemaker
import json
import time
import os
from datetime import datetime
from typing import List, Dict, Any, Optional
import numpy as np
import pandas as pd
import requests
import feedparser

# SageMaker setup
sess = sagemaker.Session()
role = sagemaker.get_execution_role()
region = sess.boto_region_name
account_id = boto3.client('sts').get_caller_identity()['Account']

print("SageMaker Session initialized")
print(f"  Region: {region}")
print(f"  Account: {account_id}")

# ═══════════════════════════════════════════════════════════════════════════
# SECTION 2: AWS ENVIRONMENT CONFIGURATION
# ═══════════════════════════════════════════════════════════════════════════

print("\n" + "="*70)
print("CONFIGURING AWS ENVIRONMENT")
print("="*70)

# Configuration - SET THESE VALUES
NGC_API_KEY = os.environ.get('NGC_API_KEY', 'YOUR_NGC_API_KEY_HERE')
SEMANTIC_SCHOLAR_KEY = os.environ.get('SEMANTIC_SCHOLAR_KEY', '')

# IMPORTANT: If you've already deployed endpoints, set them here
# Otherwise, set to None and deploy in the next section
EXISTING_LLM_ENDPOINT = "nim-llama-3-1-nemotron-nano-8b-v1"  # e.g., "nim-llama-3-1-nemotron-nano-8b-v1"
EXISTING_EMBEDDING_ENDPOINT = "nim-llama-3-2-nv-embedqa-1b-v2"  # e.g., "nim-nv-embedqa-e5-v5"

# AWS Clients
sm_client = boto3.client('sagemaker', region_name=region)
sm_runtime = boto3.client('sagemaker-runtime', region_name=region)

print("AWS clients initialized")

# ═══════════════════════════════════════════════════════════════════════════
# SECTION 3: DEPLOY OR USE EXISTING ENDPOINTS
# ═══════════════════════════════════════════════════════════════════════════

print("\n" + "="*70)
print("NVIDIA NIM ENDPOINT CONFIGURATION")
print("="*70)

# Check if using existing endpoints
if EXISTING_LLM_ENDPOINT and EXISTING_EMBEDDING_ENDPOINT:
    print("\nUsing existing endpoints:")
    llm_endpoint_name = EXISTING_LLM_ENDPOINT
    embedding_endpoint_name = EXISTING_EMBEDDING_ENDPOINT
    print(f"  LLM: {llm_endpoint_name}")
    print(f"  Embedding: {embedding_endpoint_name}")
    
    # Verify endpoints exist
    try:
        sm_client.describe_endpoint(EndpointName=llm_endpoint_name)
        print("  LLM endpoint verified")
    except:
        print("  ERROR: LLM endpoint not found!")
        
    try:
        sm_client.describe_endpoint(EndpointName=embedding_endpoint_name)
        print("  Embedding endpoint verified")
    except:
        print("  ERROR: Embedding endpoint not found!")

else:
    print("\nNO EXISTING ENDPOINTS - MANUAL DEPLOYMENT REQUIRED")
    print("\nOption 1: Deploy via AWS Marketplace")
    print("  1. Visit: https://aws.amazon.com/marketplace/search/results?searchTerms=nvidia+nim")
    print("  2. Subscribe to Llama-3.1-Nemotron-Nano-8B-v1")
    print("  3. Subscribe to NeMo Retriever Embedding NIM")
    print("  4. Deploy to SageMaker endpoints")
    print("  5. Set endpoint names below")
    
    print("\nOption 2: Use Pre-built Example Endpoints")
    print("  If you have endpoints from AWS examples, set them here:")
    
    # USER ACTION REQUIRED: Set your endpoint names here after deployment
    llm_endpoint_name = input("\nEnter your LLM endpoint name: ").strip()
    embedding_endpoint_name = input("Enter your embedding endpoint name: ").strip()
    
    if not llm_endpoint_name or not embedding_endpoint_name:
        print("\nERROR: Endpoint names required!")
        print("Please deploy endpoints first or set endpoint names above.")
        raise ValueError("Endpoint names not configured")

print("\nEndpoint Configuration Complete:")
print(f"  LLM Endpoint: {llm_endpoint_name}")
print(f"  Embedding Endpoint: {embedding_endpoint_name}")

# ═══════════════════════════════════════════════════════════════════════════
# SECTION 4: TEST ENDPOINTS
# ═══════════════════════════════════════════════════════════════════════════

print("\n" + "="*70)
print("TESTING ENDPOINTS")
print("="*70)

# Test LLM Endpoint
print("\n1. Testing LLM Endpoint...")
try:
    test_payload = {
        "model": "nvidia/llama-3.1-nemotron-nano-8b-v1",
        "messages": [
            {"role": "system", "content": "detailed thinking off"},
            {"role": "user", "content": "Say 'Hello World' in exactly 2 words."}
        ],
        "max_tokens": 50,
        "temperature": 0
    }
    
    response = sm_runtime.invoke_endpoint(
        EndpointName=llm_endpoint_name,
        ContentType="application/json",
        Body=json.dumps(test_payload)
    )
    
    result = json.loads(response['Body'].read().decode())
    print(f"SUCCESS - LLM Response: {result['choices'][0]['message']['content']}")
except Exception as e:
    print(f"ERROR - LLM: {str(e)[:300]}")
    print("\nTroubleshooting:")
    print("  - Verify endpoint name is correct")
    print("  - Check endpoint status in SageMaker console")
    print("  - Ensure endpoint is 'InService'")

# Test Embedding Endpoint
print("\n2. Testing Embedding Endpoint...")
try:
    test_embed_payload = {
        "model": "nvidia/nv-embedqa-e5-v5",
        "input": ["This is a test sentence"],
        "input_type": "query"
    }
    
    response = sm_runtime.invoke_endpoint(
        EndpointName=embedding_endpoint_name,
        ContentType="application/json",
        Body=json.dumps(test_embed_payload)
    )
    
    result = json.loads(response['Body'].read().decode())
    print(f"SUCCESS - Embedding dimension: {len(result['data'][0]['embedding'])}")
except Exception as e:
    print(f"ERROR - Embedding: {str(e)[:300]}")
    print("\nTroubleshooting:")
    print("  - Verify endpoint name is correct")
    print("  - Check endpoint status in SageMaker console")
    print("  - Ensure endpoint is 'InService'")

print("\nEndpoint testing complete!")

# ═══════════════════════════════════════════════════════════════════════════
# SECTION 5: VECTOR STORE SETUP
# ═══════════════════════════════════════════════════════════════════════════

print("\n" + "="*70)
print("SETTING UP VECTOR STORE")
print("="*70)

class SimpleVectorStore:
    """Simple in-memory vector store for demo purposes"""
    
    def __init__(self):
        self.documents = []
        self.embeddings = []
        self.metadata = []
    
    def add(self, doc_id: str, text: str, embedding: List[float], metadata: Dict):
        """Add document to vector store"""
        self.documents.append({"id": doc_id, "text": text})
        self.embeddings.append(embedding)
        self.metadata.append(metadata)
    
    def search(self, query_embedding: List[float], top_k: int = 10) -> List[Dict]:
        """Search for similar documents"""
        if not self.embeddings:
            return []
        
        query_embedding = np.array(query_embedding)
        similarities = []
        
        for i, doc_embedding in enumerate(self.embeddings):
            doc_embedding = np.array(doc_embedding)
            similarity = np.dot(query_embedding, doc_embedding) / (
                np.linalg.norm(query_embedding) * np.linalg.norm(doc_embedding)
            )
            similarities.append((i, similarity))
        
        similarities.sort(key=lambda x: x[1], reverse=True)
        
        results = []
        for idx, score in similarities[:top_k]:
            result = {
                "document": self.documents[idx],
                "metadata": self.metadata[idx],
                "score": float(score)
            }
            results.append(result)
        
        return results

vector_store = SimpleVectorStore()
print("Vector store initialized (in-memory)")

# ═══════════════════════════════════════════════════════════════════════════
# SECTION 6: RESEARCH API INTEGRATIONS
# ═══════════════════════════════════════════════════════════════════════════

print("\n" + "="*70)
print("RESEARCH API INTEGRATIONS")
print("="*70)

class ArxivAPI:
    """arXiv API integration"""
    BASE_URL = "http://export.arxiv.org/api/query"
    
    def search(self, query: str, max_results: int = 20) -> List[Dict]:
        """Search arXiv for papers"""
        params = {
            "search_query": query,
            "start": 0,
            "max_results": max_results,
            "sortBy": "relevance",
            "sortOrder": "descending"
        }
        
        response = requests.get(self.BASE_URL, params=params)
        feed = feedparser.parse(response.content)
        
        papers = []
        for entry in feed.entries:
            paper = {
                "id": entry.id.split("/")[-1],
                "title": entry.title,
                "authors": [author.name for author in entry.authors],
                "abstract": entry.summary,
                "published": entry.published,
                "pdf_url": entry.link.replace("abs", "pdf"),
                "source": "arxiv"
            }
            papers.append(paper)
        
        time.sleep(3)
        return papers

class SemanticScholarAPI:
    """Semantic Scholar API integration"""
    BASE_URL = "https://api.semanticscholar.org/graph/v1"
    
    def __init__(self, api_key: Optional[str] = None):
        self.api_key = api_key
        self.headers = {"x-api-key": api_key} if api_key else {}
    
    def search_papers(self, query: str, limit: int = 20) -> List[Dict]:
        """Search for papers"""
        params = {
            "query": query,
            "limit": limit,
            "fields": "paperId,title,abstract,authors,year,citationCount,venue"
        }
        
        response = requests.get(
            f"{self.BASE_URL}/paper/search",
            headers=self.headers,
            params=params
        )
        
        time.sleep(1)
        
        if response.status_code == 200:
            return response.json().get("data", [])
        return []

arxiv_api = ArxivAPI()
semantic_scholar_api = SemanticScholarAPI(api_key=SEMANTIC_SCHOLAR_KEY)

print("ArXiv API initialized")
print("Semantic Scholar API initialized")

# ═══════════════════════════════════════════════════════════════════════════
# SECTION 7: AGENTIC RESEARCH ASSISTANT
# ═══════════════════════════════════════════════════════════════════════════

print("\n" + "="*70)
print("AI RESEARCH ASSISTANT NAVIGATOR")
print("="*70)

class ResearchAssistantNavigator:
    """AI Research Assistant Navigator - Agentic Application"""
    
    def __init__(self, llm_endpoint: str, embedding_endpoint: str):
        self.llm_endpoint = llm_endpoint
        self.embedding_endpoint = embedding_endpoint
        self.sm_runtime = sm_runtime
        self.vector_store = vector_store
        self.arxiv = arxiv_api
        self.semantic_scholar = semantic_scholar_api
        self.conversation_history = []
    
    def embed_text(self, text: str) -> List[float]:
        """Generate embeddings using NeMo Retriever NIM"""
        payload = {
            "model": "nvidia/nv-embedqa-e5-v5",
            "input": [text],
            "input_type": "query"
        }
        
        try:
            response = self.sm_runtime.invoke_endpoint(
                EndpointName=self.embedding_endpoint,
                ContentType="application/json",
                Body=json.dumps(payload)
            )
            
            result = json.loads(response['Body'].read().decode())
            
            if 'data' in result and len(result['data']) > 0:
                return result['data'][0]['embedding']
            elif 'embeddings' in result:
                return result['embeddings'][0]
            return []
                
        except Exception as e:
            print(f"Embedding error: {str(e)[:100]}")
            return [0.0] * 1024
    
    def llm_reasoning(self, messages: List[Dict], 
                     reasoning_mode: bool = True,
                     max_tokens: int = 2048) -> str:
        """Generate response using Llama-3.1-Nemotron-Nano-8B-v1"""
        
        system_prompt = "detailed thinking on" if reasoning_mode else "detailed thinking off"
        full_messages = [{"role": "system", "content": system_prompt}]
        full_messages.extend(messages)
        
        payload = {
            "model": "nvidia/llama-3.1-nemotron-nano-8b-v1",
            "messages": full_messages,
            "max_tokens": max_tokens,
            "temperature": 0.6 if reasoning_mode else 0,
            "top_p": 0.95 if reasoning_mode else 1.0
        }
        
        try:
            response = self.sm_runtime.invoke_endpoint(
                EndpointName=self.llm_endpoint,
                ContentType="application/json",
                Body=json.dumps(payload)
            )
            
            result = json.loads(response['Body'].read().decode())
            return result['choices'][0]['message']['content']
                
        except Exception as e:
            print(f"LLM error: {str(e)[:100]}")
            return "Error in generation"
    
    def autonomous_research_query(self, user_query: str) -> Dict[str, Any]:
        """Main agentic workflow"""
        
        print("\n" + "="*70)
        print("AUTONOMOUS RESEARCH AGENT")
        print("="*70)
        print(f"Query: {user_query}\n")
        
        # Phase 1: Planning
        print("Phase 1: Query Understanding...")
        planning_prompt = f"Analyze this research query: {user_query}\nProvide key concepts and search strategy."
        messages = [{"role": "user", "content": planning_prompt}]
        plan_str = self.llm_reasoning(messages, reasoning_mode=True)
        print(f"Plan: {plan_str[:200]}...\n")
        
        # Phase 2: Literature Search
        print("Phase 2: Literature Discovery...")
        all_papers = []
        
        try:
            arxiv_papers = self.arxiv.search(user_query, max_results=10)
            all_papers.extend(arxiv_papers)
            print(f"  ArXiv: {len(arxiv_papers)} papers")
        except Exception as e:
            print(f"  ArXiv error: {str(e)[:50]}")
        
        try:
            ss_papers = self.semantic_scholar.search_papers(user_query, limit=10)
            all_papers.extend(ss_papers)
            print(f"  Semantic Scholar: {len(ss_papers)} papers")
        except Exception as e:
            print(f"  Semantic Scholar error: {str(e)[:50]}")
        
        print(f"Total: {len(all_papers)} papers\n")
        
        # Phase 3: Ranking
        print("Phase 3: Semantic Ranking...")
        query_embedding = self.embed_text(user_query)
        
        ranked_papers = []
        for paper in all_papers[:20]:
            paper_text = f"{paper.get('title', '')} {paper.get('abstract', '')}"
            paper_embedding = self.embed_text(paper_text[:500])
            
            if paper_embedding and len(paper_embedding) > 0:
                similarity = np.dot(query_embedding, paper_embedding) / (
                    np.linalg.norm(query_embedding) * np.linalg.norm(paper_embedding)
                )
                paper['relevance_score'] = float(similarity)
                ranked_papers.append(paper)
        
        ranked_papers.sort(key=lambda x: x.get('relevance_score', 0), reverse=True)
        top_papers = ranked_papers[:10]
        print(f"Top {len(top_papers)} papers selected\n")
        
        # Phase 4: Synthesis
        print("Phase 4: Literature Synthesis...")
        synthesis_prompt = f"Synthesize findings from research on: {user_query}\nProvide main themes and key insights."
        messages = [{"role": "user", "content": synthesis_prompt}]
        synthesis = self.llm_reasoning(messages, reasoning_mode=True, max_tokens=1500)
        
        print("\nRESEARCH COMPLETE!")
        
        return {
            "query": user_query,
            "papers_found": len(all_papers),
            "top_papers": top_papers,
            "synthesis": synthesis,
            "timestamp": datetime.now().isoformat()
        }

# Initialize Research Assistant
print("\nInitializing Research Assistant...")
research_assistant = ResearchAssistantNavigator(
    llm_endpoint=llm_endpoint_name,
    embedding_endpoint=embedding_endpoint_name
)
print("Research Assistant ready!")

# ═══════════════════════════════════════════════════════════════════════════
# SECTION 8: RUN DEMO
# ═══════════════════════════════════════════════════════════════════════════

print("\n" + "="*70)
print("RUNNING DEMO")
print("="*70)

# Example research query
demo_query = "transformer architectures for protein structure prediction"

print(f"\nDemo Query: {demo_query}")
results = research_assistant.autonomous_research_query(demo_query)

# Display results
print("\n" + "="*70)
print("RESULTS")
print("="*70)

print(f"\nPapers Found: {results['papers_found']}")
print(f"Top Relevant Papers: {len(results['top_papers'])}")

print("\nTop 5 Papers:")
for i, paper in enumerate(results['top_papers'][:5], 1):
    print(f"\n{i}. {paper.get('title', 'Untitled')}")
    print(f"   Relevance: {paper.get('relevance_score', 0):.3f}")
    print(f"   Source: {paper.get('source', 'Unknown')}")

print("\n" + "-"*70)
print("SYNTHESIS:")
print("-"*70)
print(results['synthesis'])

# Export results
output_file = f"research_results_{int(time.time())}.json"
with open(output_file, 'w') as f:
    json.dump(results, f, indent=2, default=str)

print(f"\nResults saved to: {output_file}")
print("\nDEMO COMPLETE!")
